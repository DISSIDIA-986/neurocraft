"Front","Back"
"如何为不同类型的数据选择合适的机器学习算法？","**结构化数据**: 随机森林, XGBoost, SVM。 **图像数据**: CNN, Vision Transformer。 **文本/序列数据**: RNN, LSTM, Transformer。 **小数据集**: 线性模型, SVM, 基于树的方法。 **大数据集**: 深度学习。"
"在选择机器学习模型时，为什么通常建议从简单模型开始？","从简单模型（如线性回归）开始可以快速建立一个性能基线。这有助于判断问题的难度，并评估复杂模型带来的性能提升是否值得其增加的复杂性和计算成本。"
"什么是scikit-learn？它主要适用于什么场景？","scikit-learn是一个非常流行的Python机器学习库，它提供了大量经典机器学习算法的简单高效的工具。它非常适合用于中小型数据集的分析、快速原型开发和教学。"
"PyTorch和TensorFlow是两种主流的深度学习框架，它们各有什么特点？","**PyTorch**以其灵活性和易用性著称（动态计算图），在学术研究和快速原型设计中非常流行。**TensorFlow**则以其强大的生态系统和生产部署能力（如TensorFlow Serving, TensorFlow Lite）见长，在工业界应用广泛。"
"XGBoost是什么？它在哪类问题上表现出色？","XGBoost是一个高效、灵活且可移植的梯度提升（Gradient Boosting）库。它在处理结构化或表格化数据方面表现极其出色，是许多数据科学竞赛（如Kaggle）中的获胜法宝。"
"Keras是什么？它与TensorFlow是什么关系？","Keras是一个用户友好的高级神经网络API，旨在简化和加速深度学习模型的原型设计。它以其简洁和模块化的接口而闻名。从TensorFlow 2.0开始，Keras已被正式集成为TensorFlow的官方高级API。"
"Hugging Face Transformers库的主要作用是什么？","Hugging Face Transformers库提供了一个标准化的接口，让开发者可以轻松地下载、使用和微调数以千计的基于Transformer架构的预训练模型（如BERT, GPT, T5）。它极大地简化了现代NLP（自然语言处理）模型的应用。"
"在机器学习项目中，特征工程和模型选择哪个更重要？","通常认为特征工程（Feature Engineering）更重要。一个好的特征集可以使简单的模型表现出色，而一个差的特征集即使使用最复杂的模型也可能效果不佳。所谓“Garbage in, garbage out”。"
"什么是集成学习 (Ensemble Learning)？为什么它通常能提高性能？","集成学习是一种通过构建并结合多个学习器来完成学习任务的技术。例如随机森林和梯度提升。它通常能提高性能，因为通过结合多个不同模型的“观点”，可以减少单一模型的偏差和方差，从而获得更稳健、更准确的预测。"
