"Front","Back","TAGS"
"什么是Transformer架构？它是由谁在哪篇论文中提出的？","Transformer是一种于2017年提出的深度学习架构，它完全基于自注意力机制，摒弃了以往序列处理模型中的循环（RNN）和卷积（CNN）结构。它由Google团队在著名论文《Attention Is All You Need》中提出。","Transformers"
"Transformer相比于RNN在处理序列数据时最大的优势是什么？","最大的优势是**并行计算能力**。RNN必须按顺序逐个处理序列中的元素，而Transformer可以同时处理整个序列的所有元素，这极大地提高了训练效率，使得训练超大规模模型成为可能。","Transformers"
"Transformer是如何解决长距离依赖问题的？","通过自注意力机制 (Self-Attention)。该机制可以直接计算序列中任意两个位置之间的依赖关系，无论它们相距多远，信息传递的路径长度都是O(1)。这与RNN中信息需要逐-步传递、容易丢失的情况形成鲜明对比。","Transformers"
"既然Transformer没有循环结构，它是如何理解序列中单词的顺序的？","通过引入**位置编码 (Positional Encoding)**。这是一种特殊的向量，它被加到每个输入词的嵌入向量上。位置编码为模型提供了关于单词在序列中绝对或相对位置的信息，从而让模型能够区分顺序。","Transformers"
"Transformer中的“多头注意力 (Multi-Head Attention)”机制有什么作用？","它允许模型在不同的表示子空间中并行地学习信息。简单来说，就是让模型可以同时从多个不同的“角度”去关注输入序列的不同部分，从而捕捉到更丰富的特征和关系。例如，一个“头”可能关注语法关系，另一个“头”可能关注语义关系。","Transformers"
"Transformer的编码器 (Encoder) 和解码器 (Decoder) 各自的作用是什么？","**编码器**负责处理和理解整个输入序列，生成一系列富含上下文信息的表示（representation）。**解码器**则负责根据编码器的输出和已经生成的部分内容，自回归地（一个接一个地）生成输出序列。","Transformers"
"在Transformer解码器中，“掩码自注意力 (Masked Self-Attention)”的作用是什么？","在生成输出序列的第 `i` 个词时，解码器应该只能看到位置 `i` 之前的词，而不能“偷看”未来的词。掩码自注意力的作用就是屏蔽掉（mask out）未来位置的信息，以确保模型的自回归属性。","Transformers"
"什么是BERT和GPT？它们在Transformer架构的使用上有何不同？","它们都是基于Transformer的著名预训练模型。**BERT**主要使用Transformer的**编码器**部分，它通过掩码语言模型（Masked Language Model）任务来学习双向的上下文表示，非常适合文本理解任务（如分类、问答）。**GPT**主要使用Transformer的**解码器**部分，它通过标准的从左到右的语言模型任务进行训练，非常适合文本生成任务。","Transformers"
"Transformer架构面临的主要挑战是什么？","主要是其自注意力机制的计算复杂度。对于长度为 `n` 的序列，标准自注意力的计算和内存复杂度都是 O(n²)，这使得它在处理非常长的序列（如长文档、高分辨率图像）时变得非常昂贵和低效。","Transformers"
"什么是Vision Transformer (ViT)？","ViT是将Transformer架构成功应用于计算机视觉领域的模型。它通过将图像分割成一个个小的图像块（patches），并将这些图像块序列化，然后像处理单词一样将它们输入到标准的Transformer编码器中，从而进行图像分类等任务。","Transformers"