"Front","Back","TAGS"
"What is the key difference between a CNN and an RNN?","**CNNs** are like specialists for **spatial data** (e.g., images), using filters to find patterns across a grid. **RNNs** are specialists for **sequential data** (e.g., text, time series), using a loop to maintain a 'memory' of past information to influence the present. Think of CNNs as having sharp eyes and RNNs as having a good memory.","DeepLearning"
"Why was the Long Short-Term Memory (LSTM) network developed?","Standard RNNs suffer from 'short-term memory.' They struggle to remember information from many steps ago, a problem known as the vanishing gradient problem. **LSTMs** were designed to fix this. They are a special type of RNN with a more complex memory cell, using 'gates' to carefully control what information to remember, what to forget, and what to output, allowing them to learn long-range dependencies.","DeepLearning"
"What problem does a Residual Network (ResNet) solve?","It solves the 'degradation' problem in very deep networks. Paradoxically, just stacking more layers could make a network perform *worse*. ResNet introduces 'skip connections' or 'shortcuts' that allow the gradient to flow more easily through the network during training. This allows for the successful training of networks that are hundreds or even thousands of layers deep.","DeepLearning"
"What is the primary purpose of the Transformer architecture?","The Transformer was designed to process sequential data, like RNNs, but without the sequential processing bottleneck. Its primary purpose is to handle long-range dependencies more effectively and to allow for massive **parallelization** by using the self-attention mechanism, which made training today's large language models feasible.","DeepLearning"
