"Front","Back","TAGS"
"What is a Word Embedding?","A word embedding is a dense vector representation of a word. Instead of representing a word with a sparse, high-dimensional vector (like one-hot encoding), it uses a lower-dimensional, floating-point vector that captures the word's semantic meaning.","Embeddings"
"What is the core principle behind word embedding algorithms like Word2Vec?","The **Distributional Hypothesis**: 'You shall know a word by the company it keeps.' These algorithms learn a word's meaning by analyzing the context (the words that appear around it) in a massive amount of text. Words that appear in similar contexts will have similar embedding vectors.","Embeddings"
"What famous analogy demonstrates the power of word embeddings?","The 'King - Man + Woman â‰ˆ Queen' analogy. This shows that embeddings can capture complex semantic relationships, like gender and royalty, as directions in the vector space. You can perform arithmetic operations on word meanings.","Embeddings"
"What is the main limitation of classic word embeddings like Word2Vec or GloVe?","They are **static** and cannot handle polysemy (words with multiple meanings). For example, the word 'bank' would have the exact same vector in 'river bank' and 'investment bank,' failing to capture the different contexts.","Embeddings"
"How do modern contextual embeddings (from models like BERT or GPT) solve the limitation of static embeddings?","Contextual embedding models generate a word's vector based on the *entire sentence* it appears in. Therefore, the vector for 'bank' in 'river bank' will be different from the vector for 'bank' in 'investment bank,' because the surrounding words provide the necessary context to disambiguate its meaning.","Embeddings"
