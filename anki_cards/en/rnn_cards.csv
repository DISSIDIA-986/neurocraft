"Front","Back","TAGS"
"What is a Recurrent Neural Network (RNN)?","An RNN is a type of neural network designed for **sequential data** (like text or time series). Its defining feature is a 'loop,' which allows it to maintain a 'memory' or 'state' of past information to influence its processing of current information.","RNN"
"Use an analogy to explain how an RNN works.","Imagine reading a sentence one word at a time. As you read, you keep a running summary of what you've read so far in your head. An RNN does this by passing a **hidden state** (the summary) from one step to the next. The hidden state at each step is a combination of the current word and the summary from the previous word.","RNN"
"What is the 'vanishing gradient problem' in RNNs?","This is the biggest weakness of simple RNNs. When training on long sequences, the error signal (gradient) has to be propagated back through many time steps. This signal can get progressively smaller until it 'vanishes,' meaning the network can't learn from events that happened long ago. This gives RNNs a very short-term memory.","RNN"
"How did LSTMs and GRUs solve the vanishing gradient problem?","They introduced **gates**â€”special mechanisms that act like controllers for the network's memory. These gates learn to selectively 'forget' irrelevant old information and 'remember' important new information, creating a more robust memory system that allows gradients to flow over much longer sequences.","RNN"
"What is a Bidirectional RNN?","A Bidirectional RNN processes a sequence in two directions at once: one RNN moves forward (from start to end), and another moves backward (from end to start). The outputs from both are then combined. This gives the network a complete 'past' and 'future' context for every point in the sequence, which is very useful for tasks like sentiment analysis where the end of a sentence can change the meaning of the beginning.","RNN"
"Why have Transformers largely replaced RNNs for NLP tasks?","For two main reasons: 1. **Parallelization:** RNNs are inherently sequential and slow to train. Transformers can process all words in a sequence at once. 2. **Long-Range Dependencies:** Transformers' self-attention mechanism is much better at modeling direct relationships between distant words in a sequence than RNNs are.","RNN"
