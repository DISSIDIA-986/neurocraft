"Front","Back","TAGS"
"What was the main breakthrough of the Transformer architecture, as introduced in 'Attention Is All You Need'?","The main breakthrough was demonstrating that a network architecture based **solely on the attention mechanism**, without any recurrent or convolutional layers, could achieve state-of-the-art performance on sequence-to-sequence tasks. This enabled massive parallelization and more effective capturing of long-range dependencies.","Transformers"
"How does a Transformer know the order of words in a sentence without an RNN's sequential structure?","It uses **Positional Encodings**. Before the word embeddings are fed into the first layer, a special vector that represents the position of each word (e.g., 1st, 2nd, 3rd) is added to them. This injects information about the sequence order directly into the word representations.","Transformers"
"What is the role of the Encoder and Decoder in the original Transformer model?","- The **Encoder**'s job is to read and understand the entire input sentence, building a rich, context-aware representation of each word. - The **Decoder**'s job is to generate the output sentence word by word, while paying attention to both the Encoder's output and the words it has already generated itself.","Transformers"
"What is the difference in how BERT and GPT use the Transformer architecture?","- **BERT** ('Bidirectional Encoder Representations from Transformers') uses only the **Encoder** part of the Transformer. It's designed to be deeply bidirectional, making it excellent for understanding tasks like classification. - **GPT** ('Generative Pre-trained Transformer') uses only the **Decoder** part. It's an autoregressive model, making it excellent for generation tasks.","Transformers"
"What is 'masked self-attention' and where is it used?","Masked self-attention is used in the **Decoder** of the Transformer. When the Decoder is generating a new word, it should only be allowed to pay attention to the words that came before it. The mask effectively 'hides' all future words, preventing the model from 'cheating' by looking ahead in the sequence.","Transformers"
"What is a Vision Transformer (ViT)?","A ViT adapts the Transformer architecture for image processing. It works by breaking an image down into a sequence of fixed-size patches (like turning an image into a sentence of 'image words'). It then feeds this sequence of patches into a standard Transformer encoder to perform tasks like image classification.","Transformers"
