"Front","Back","TAGS"
"What is a Multi-Layer Perceptron (MLP)?","An MLP is the classic form of a feedforward neural network. It consists of an input layer, one or more 'hidden' layers, and an output layer, where each layer is fully connected to the next. It's the foundational architecture for many more complex deep learning models.","MLP"
"What was the key limitation of the original Perceptron, and how did MLPs solve it?","The original single-layer Perceptron could only learn **linearly separable** patterns. It famously could not solve the simple XOR problem. MLPs solve this by introducing **hidden layers** and **non-linear activation functions**, which allow them to learn and represent complex, non-linear relationships in data.","MLP"
"What is the role of an activation function in an MLP?","An activation function introduces non-linearity into the network. Without it, no matter how many layers you stack, the MLP would just be performing a series of linear operations, making it equivalent to a simple linear model. Activation functions (like ReLU or sigmoid) are what give neural networks their power to approximate any complex function.","MLP"
"What is 'Forward Propagation' in an MLP?","It's the process of data flowing through the network. The input data is fed into the first layer, and the outputs of that layer are fed as inputs to the next layer, and so on, until the final output is produced at the output layer. It's a one-way street from input to output.","MLP"
"What does it mean for a layer to be 'fully connected' (or 'dense')?","It means that every neuron in that layer is connected to *every single neuron* in the next layer. Each of these connections has its own unique weight. This is the standard type of layer used in an MLP.","MLP"
