"Front","Back","TAGS"
"What is Tokenization in NLP?","Tokenization is the first step in NLP: breaking down a stream of text into smaller units called 'tokens.' These tokens can be words, characters, or sub-words. For example, the sentence 'I love NLP' is tokenized into the tokens ['I', 'love', 'NLP'].","NLP"
"What are Stop Words, and why are they often removed?","Stop words are extremely common words that carry little semantic meaning, such as 'the,' 'a,' 'is,' or 'in.' They are often removed from text to reduce noise and dimensionality, allowing the model to focus on the more important, meaning-bearing words.","NLP"
"What is the difference between Stemming and Lemmatization?","Both aim to reduce words to their root form, but they do it differently. **Stemming** is a crude, rule-based process that just chops off word endings (e.g., 'studies,' 'studying' -> 'studi'). It's fast but can produce non-real words. **Lemmatization** is a more sophisticated, dictionary-based process that considers the word's context and part of speech to return its true root form, or 'lemma' (e.g., 'studies,' 'studying' -> 'study'; 'better' -> 'good'). It's more accurate but slower.","NLP"
"What is Part-of-Speech (POS) Tagging?","POS Tagging is the process of reading text and assigning a part of speech—such as noun, verb, adjective, etc.—to each word. This is a fundamental step for understanding the grammatical structure of a sentence.","NLP"
"What is a typical NLP pre-processing pipeline?","A standard pipeline involves a sequence of cleaning steps to prepare text for a model: 1. Lowercasing the text. 2. Tokenization (splitting text into words). 3. Stop Word Removal. 4. Stemming or Lemmatization (reducing words to their roots). This cleaned sequence of tokens is then typically converted into numerical vectors for the model to process.","NLP"
