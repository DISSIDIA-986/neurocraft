"Front","Back","TAGS"
"What is the core idea of the Attention Mechanism in deep learning?","Imagine you're at a noisy party. Your brain automatically 'pays attention' to your friend's voice while filtering out the background noise. The Attention Mechanism does the same for a model: it acts like a dynamic 'spotlight,' allowing the model to focus on the most relevant parts of the input data while ignoring the rest. It does this by assigning different 'weights' or importance scores to each piece of input.","Attention"
"What are the roles of Query (Q), Key (K), and Value (V) in the Attention mechanism?","Think of it like searching for a video on YouTube. Query (Q) is your search text (what you're looking for). Keys (K) are the video titles (used for matching). Values (V) are the actual videos. The model matches your Q with all Ks to decide how much attention to pay to each corresponding V.","Attention"
"What is the difference between Self-Attention and Cross-Attention?","**Self-Attention** is like introspection; it helps words within a *single sentence* understand each other's context (e.g., linking 'it' to 'the cat'). **Cross-Attention** is like a conversation; it connects *two different sequences*, such as a decoder paying attention to a source sentence during translation.","Attention"
"Why is Multi-Head Attention used instead of a single attention layer?","Using a single attention layer is like having only one perspective. Multi-Head Attention runs multiple attention mechanisms in parallel (each 'head' is a different perspective), allowing the model to capture different types of relationships from various 'angles' simultaneously for a richer understanding.","Attention"
"What is the main advantage of the Transformer architecture over RNNs?","**Parallelization**. RNNs must process a sentence word-by-word, which is slow. Transformers can process all words in a sentence at the same time using self-attention, making training on massive datasets much faster and more efficient.","Attention"
"What is the primary drawback of the original Attention mechanism?","Its computational complexity is O(nÂ²), where 'n' is the sequence length. This means the computation and memory cost grows quadratically as the sequence gets longer, making it very expensive for long documents or high-resolution images.","Attention"
