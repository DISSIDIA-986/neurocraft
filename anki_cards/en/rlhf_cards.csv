"Front","Back","TAGS"
"What is RLHF (Reinforcement Learning from Human Feedback)?","RLHF is a technique used to 'align' a language model with human values. It uses human preferences as a reward signal to fine-tune the model, encouraging it to generate outputs that are not just grammatically correct, but also helpful, honest, and harmless.","RLHF"
"What are the three main steps of the RLHF process?","1. **Supervised Fine-Tuning (SFT):** First, fine-tune a pre-trained LLM on a small, high-quality dataset of human-written demonstrations. 2. **Train a Reward Model:** Collect human preference data by having them rank different model outputs, then train a separate model (the Reward Model) to predict which outputs humans would prefer. 3. **Reinforcement Learning:** Use the trained Reward Model as a reward function to further fine-tune the LLM with a reinforcement learning algorithm (like PPO).","RLHF"
"Use an analogy to explain how the RLHF reward model works.","Imagine training a dog. You can't explain complex rules, but you can give it a 'good dog' treat when it does something you like. The **Reward Model** is like your internal 'treat-giving' instinct. It learns to look at a model's response and predict how many 'treats' (preference score) a human would give it.","RLHF"
"In the final RL step of RLHF, what is the LLM being 'reinforced' to do?","The LLM is reinforced to generate responses that will get a high score from the **Reward Model**. In essence, the LLM is learning to optimize its behavior to please the Reward Model, which itself is a proxy for human preferences.","RLHF"
"What is the purpose of the KL Divergence penalty in the RLHF process?","It acts as a 'leash.' While the reinforcement learning step tries to pull the model towards generating answers that get a high reward, the KL penalty pulls it back towards its original, supervised-tuned behavior. This prevents the model from 'over-optimizing' on the reward and generating bizarre, unnatural text just to get a high score. It helps maintain linguistic quality and diversity.","RLHF"
"What is the main challenge or drawback of RLHF?","The biggest challenge is the **quality and cost of data collection**. The entire process relies on human feedback, which is expensive to gather, can be inconsistent, and may contain the biases of the human labelers. The final model is only as good and as aligned as the preferences it was trained on.","RLHF"
