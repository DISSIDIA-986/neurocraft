"Front","Back","TAGS"
"What is the core concept of an Autoencoder?","It's a 'compression and reconstruction' network. An **Encoder** compresses data (like zipping a file) to capture its most important features, and a **Decoder** tries to reconstruct the original data from that compressed version. The goal is to learn a meaningful summary of the data.","AutoEncoder"
"How are Autoencoders used for Anomaly Detection?","Imagine an art forger trained only on authentic paintings. They can spot a fake instantly because it looks 'wrong.' An autoencoder trained only on *normal* data works the same way. It perfectly reconstructs normal data (low error), but fails to reconstruct an *anomaly* (high error), flagging it as a fake.","AutoEncoder"
"What's the key difference between a regular Autoencoder and a Variational Autoencoder (VAE)?","A regular Autoencoder learns a single 'point' for each input's summary. A **VAE** learns a 'region' or probability distribution. This is crucial because it allows VAEs to **generate new data** by sampling from these learned regions, a trick regular autoencoders can't do.","AutoEncoder"
"What is a Denoising Autoencoder?","It's a network trained to be a 'clean-up' artist. You feed it a corrupted or 'noisy' image, and its job is to output the original, clean version. This forces it to learn robust features and ignore irrelevant noise.","AutoEncoder"
"What is the 'bottleneck' in an Autoencoder?","The bottleneck is the central, narrowest layer where the compressed data representation exists. It's the 'choke point' that forces the network to learn an efficient summary, as it can't just copy the input to the output.","AutoEncoder"