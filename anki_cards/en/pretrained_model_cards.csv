"Front","Back","TAGS"
"What is a Pre-trained Model?","A pre-trained model is a neural network that has already been trained on a very large, general dataset (like ImageNet for images or Wikipedia for text). It has already learned a rich set of features and can be used as a powerful starting point for new, related tasks.","PretrainedModel"
"What is the main benefit of using a pre-trained model?","It saves a massive amount of time and computational resources. Instead of training a network from scratch, which can take days or weeks, you start with a model that already understands the world, allowing you to achieve high performance on your specific task with much less data and training time.","PretrainedModel"
"What is Transfer Learning?","Transfer learning is the process of taking a pre-trained model and adapting it to a new, different but related task. It's like a chef who has mastered general cooking techniques (pre-training) and then quickly learns to cook a new, specific dish (the new task).","PretrainedModel"
"What are the two main strategies for using a pre-trained model?","1. **Feature Extraction:** Use the pre-trained model as a fixed feature extractor. You freeze its weights and only train a new, small classifier on top of it. This is fast and good for small datasets. 2. **Fine-Tuning:** Unfreeze the top few layers of the pre-trained model and train them with a low learning rate on your new data. This fine-tunes the learned features to be more specific to your task.","PretrainedModel"
"When should you choose Fine-Tuning over Feature Extraction?","You should choose **Fine-Tuning** when you have a relatively large dataset for your target task. This provides enough data to adjust the pre-trained weights without overfitting. If your dataset is very small, **Feature Extraction** is a safer and faster option.","PretrainedModel"
"What is Parameter-Efficient Fine-Tuning (PEFT)?","PEFT methods (like LoRA) are techniques for adapting large pre-trained models without updating all of their parameters. Instead of fine-tuning billions of weights, you only train a small number of new, added parameters. This dramatically reduces the memory and computational cost of fine-tuning, making it possible on consumer hardware.","PretrainedModel"
