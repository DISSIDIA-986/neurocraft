"Front","Back","TAGS"
"What is Retrieval-Augmented Generation (RAG)?","RAG is a technique that enhances a Large Language Model (LLM) by first retrieving relevant information from an external knowledge base and then providing that information as context to the LLM to generate a more accurate and informed answer.","RAG"
"What two major problems with LLMs does RAG help solve?","1. **Hallucination:** It grounds the model in factual, retrieved data, reducing the chance that the LLM will make things up. 2. **Outdated Knowledge:** It allows the LLM to access up-to-date information from an external source, overcoming the knowledge cut-off of its training data.","RAG"
"Use an analogy to explain the RAG workflow.","Imagine you ask an expert a question. Instead of answering from memory alone (standard LLM), the expert first does a quick search in a library for the most relevant books (Retrieval). They then read the key passages from those books and use that information to formulate a comprehensive and accurate answer for you (Augmented Generation).","RAG"
"What are the three main steps in a RAG pipeline?","1. **Indexing:** Documents are loaded, split into chunks, converted into vector embeddings, and stored in a vector database. 2. **Retrieval:** The user's query is converted into an embedding and used to find the most similar document chunks from the vector database. 3. **Generation:** The original query and the retrieved chunks are fed into an LLM, which generates the final answer based on the provided context.","RAG"
"What is a Vector Database and what is its role in RAG?","A vector database is a specialized database designed to efficiently store and search through a large number of vector embeddings. In RAG, it acts as the indexed 'library,' allowing for a very fast semantic search to find the document chunks that are most relevant to a user's query, not just by keyword matching, but by meaning.","RAG"
"Why is 'chunking' a critical step in RAG?","Chunking is the process of breaking large documents into smaller, manageable pieces. It's critical because chunks that are **too large** might contain too much noise, while chunks that are **too small** might not contain enough context. Finding the right chunking strategy is key to balancing relevance and context.","RAG"
