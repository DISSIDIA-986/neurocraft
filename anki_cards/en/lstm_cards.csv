"Front","Back","TAGS"
"What is the primary problem with standard RNNs that LSTMs were designed to solve?","Standard RNNs suffer from the **vanishing gradient problem**, which makes them have 'short-term memory.' They struggle to learn dependencies between events that are far apart in a sequence. LSTMs were created to solve exactly this, allowing them to remember information for long periods.","LSTM"
"What is the 'cell state' in an LSTM?","The cell state is the 'memory' of the LSTM. Think of it as a conveyor belt running through the entire network. The LSTM can carefully add information to this conveyor belt or remove information from it using 'gates,' allowing important context from long ago to be carried forward to the present.","LSTM"
"What are the three 'gates' in an LSTM cell and what do they do?","LSTMs have three gates that act as controllers for its memory: 1. **Forget Gate:** Decides what old information to throw away from the cell state. 2. **Input Gate:** Decides what new information to store in the cell state. 3. **Output Gate:** Decides what part of the cell state to use for the current time step's output.","LSTM"
"Use an analogy to explain the function of LSTM gates.","Imagine you're reading a novel. Your brain (the LSTM) uses gates to manage the plot: A new character is introduced, so the **Input Gate** opens to add them to your memory (cell state). A character dies, so the **Forget Gate** opens to remove them. Someone asks what's happening *right now*, and the **Output Gate** selects the relevant current events from your memory to form an answer.","LSTM"
"What is a GRU (Gated Recurrent Unit) and how does it relate to an LSTM?","A GRU is a newer, simplified version of an LSTM. It also uses gates to control information flow but combines the forget and input gates into a single 'update gate' and merges the cell state and hidden state. It's generally faster to train and performs similarly to LSTMs on many tasks, making it a popular alternative.","LSTM"
