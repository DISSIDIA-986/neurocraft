"Front","Back"
"什么是词嵌入 (Word Embedding)？","词嵌入是一种将词汇表中的单词映射到由实数构成的向量的技术。它是自然语言处理（NLP）中表示单词的一种方法，相比于传统的独热编码（one-hot encoding），它能更有效地捕捉单词的语义和句法关系。"
"词嵌入相比于独热编码（one-hot encoding）有什么优势？","独热编码向量维度高、稀疏，且无法表示单词间的关系（任意两个向量的点积都为0）。词嵌入向量维度低、稠密，并且能够通过向量空间中的距离和方向来表示单词之间的语义相似性和关系。"
"词嵌入是如何捕捉单词之间的语义关系的？","语义上相似的单词在向量空间中的位置会更接近。例如，“猫”和“狗”的向量会比“猫”和“汽车”的向量更近。"
"词嵌入如何通过向量运算来表示类比关系？","词嵌入向量可以进行有意义的代数运算。最经典的例子是 ""king"" - ""man"" + ""woman"" ≈ ""queen""，这表明模型学习到了性别和皇室地位这两个维度的关系。"
"训练词嵌入的基本思想是什么？","基于“分布式假设”（Distributional Hypothesis），即“上下文相似的单词，其语义也相似”。模型通过分析一个单词周围的上下文单词来学习该单词的向量表示。"
"有哪些著名的词嵌入算法？","Word2Vec (由Google提出，包含CBOW和Skip-gram两种模型)、GloVe (由斯坦福大学提出，结合了全局矩阵分解和局部上下文窗口的优点) 和 FastText (由Facebook提出，引入了子词信息来处理罕见词和未登录词)。"
"静态词嵌入（如Word2Vec）和上下文词嵌入（如BERT）有什么区别？","静态词嵌入为每个单词提供一个固定的向量，无法处理一词多义问题（如 ""bank"" 可以是银行或河岸）。上下文词嵌入则根据单词在句子中的具体上下文动态生成其向量表示，因此能更好地区分多义词。"
