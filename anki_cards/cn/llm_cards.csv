"Front","Back","TAGS"
"什么是大语言模型 (LLM)？","大语言模型（LLM）是基于深度学习技术（通常是Transformer架构）训练的超大规模神经网络模型，它包含数十亿甚至数千亿个参数，在海量的文本数据上进行预训练，从而能够理解和生成人类语言。","LLM"
"LLM的“涌现能力 (Emergent Abilities)”是指什么？","指当模型规模（如参数量）达到某个临界点后，会突然出现一些在小模型上不存在、也未被明确训练过的新能力，例如算术计算、逻辑推理、上下文学习等。","LLM"
"LLM的训练过程通常包含哪几个关键阶段？","1. **数据准备**：收集和清洗海量文本数据。 2. **预训练 (Pre-training)**：在无标注数据上进行自监督学习（如预测下一个词）。 3. **监督微调 (SFT)**：在高质量的指令-回答数据对上微调，使其学会遵循指令。 4. **人类反馈强化学习 (RLHF)**：通过人类偏好反馈进一步对齐模型行为，使其更有用、更安全。","LLM"
"什么是提示工程 (Prompt Engineering)？","提示工程是一门设计和优化输入文本（即“提示”，Prompt）的艺术和科学，旨在更好地引导大语言模型生成期望的、高质量的输出。","LLM"
"什么是上下文学习 (In-Context Learning) 或少样本学习 (Few-shot Learning)？","指LLM能够通过在提示中提供几个任务示例（shots），直接学会如何执行一个新任务，而无需更新其网络权重（即无需重新训练或微调）。","LLM"
"什么是思维链 (Chain-of-Thought, CoT) 提示？","思维链是一种高级提示技术，它通过在提示中向模型展示一步一步解决问题的推理过程，来引导模型在回答复杂问题（如数学或逻辑题）时也进行类似的逐步推理，从而显著提高其准确性。","LLM"
"LLM的“幻觉 (Hallucination)”问题是什么？","“幻觉”是指LLM生成看似合理、充满自信，但实际上是错误的、虚构的或与事实不符的内容。这是LLM应用中的一个主要挑战。","LLM"
"什么是LLM的对齐 (Alignment)？","对齐是指通过一系列技术（如SFT和RLHF），使LLM的行为和输出符合人类的价值观、意图和偏好，确保模型是有帮助的（Helpful）、诚实的（Honest）和无害的（Harmless）。","LLM"
"什么是规模定律 (Scaling Laws)？","规模定律描述了LLM的性能与其三个关键因素——模型规模（参数量）、训练数据量和计算量——之间存在的可预测的幂律关系。简单来说，增加这三个因素通常会带来更好的模型性能。","LLM"
"Chinchilla定律对LLM训练有什么启示？","Chinchilla定律指出，为了在给定的计算预算下获得最佳性能，模型参数量和训练数据量应该按比例增加。这意味着，许多早期的大模型可能“参数过多而数据不足”，通过增加数据量可以训练出性能更好但规模更小的模型。","LLM"
"开源LLM（如LLaMA, Mistral）和闭源LLM（如GPT-4, Claude）各有什么特点？","**闭源LLM**通常性能最强，由大公司控制，通过API提供服务。**开源LLM**允许研究者和开发者自由访问、修改和部署模型，促进了社区创新和定制化应用，但通常性能略逊于最顶尖的闭源模型。","LLM"
"LLM面临哪些主要的局限和挑战？","幻觉、知识截止（信息不是最新的）、偏见和歧视、高昂的计算成本、安全风险（被用于恶意目的）、以及可解释性差等。","LLM"