"Front","Back","TAGS"
"什么是循环神经网络 (RNN)？","RNN是一类专门用于处理序列数据的神经网络。它的核心特征是具有“循环连接”，允许信息从一个时间步传递到下一个时间步，从而创建一种“记忆”机制。","RNN"
"RNN的“记忆”是通过什么机制实现的？","通过隐藏状态 (Hidden State)。在每个时间步，RNN接收当前的输入和前一个时间步的隐藏状态，然后计算出当前时间步的输出和新的隐藏状态。这个新的隐藏状态会被传递到下一个时间步，从而实现了信息的持久化和传递。","RNN"
"RNN中的“权重共享 (Weight Sharing)”是什么意思？","在RNN中，处理序列中每个元素所使用的神经网络层（包括权重和偏置）都是相同的。这意味着模型在整个序列的不同位置使用同一套参数，这大大减少了需要学习的参数数量，并使得模型能够处理可变长度的序列。","RNN"
"RNN训练中面临的最主要挑战是什么？","梯度消失/爆炸 (Vanishing/Exploding Gradients) 问题。由于RNN的反向传播过程（BPTT）需要沿时间步链式传播梯度，当序列很长时，梯度可能会指数级地缩小（消失）或增大（爆炸），导致模型难以学习到序列中的长期依赖关系。","RNN"
"什么是BPTT (Backpropagation Through Time)？","BPTT是用于训练RNN的反向传播算法。它首先将RNN按时间步展开成一个深层的前馈神经网络，然后应用标准的反向传播算法来计算梯度。","RNN"
"LSTM和GRU是如何解决RNN的长期依赖问题的？","LSTM（长短期记忆网络）和GRU（门控循环单元）都是RNN的变体。它们通过引入“门控机制”来更精细地控制隐藏状态中信息的流动。这些门可以学习何时遗忘旧信息、何时添加新信息，从而使得重要的信息可以在长序列中更有效地传递，缓解了梯度消失问题。","RNN"
"什么是双向RNN (Bidirectional RNN)？","双向RNN由两个独立的RNN组成，它们一个按正向顺序处理序列，另一个按反向顺序处理序列。在每个时间步，两个RNN的隐藏状态会被拼接起来作为最终的输出表示。这使得模型在做预测时能够同时利用过去（前文）和未来（后文）的上下文信息。","RNN"
"为什么现在很多NLP任务都使用Transformer而不是RNN？","主要有两个原因：1. **并行计算**：RNN必须按顺序处理序列，无法并行化，训练效率低。而Transformer基于自注意力机制，可以并行处理整个序列。 2. **长距离依赖**：RNN难以捕捉非常长的依赖关系，而Transformer的自注意力机制可以直接计算序列中任意两个位置之间的关系，更擅长此道。","RNN"
"RNN有哪些典型的应用场景？","语音识别、时间序列预测（如股票价格、天气）、机器翻译（早期的Seq2Seq模型）、文本生成、手写识别和视频内容分析等。尽管在NLP领域被Transformer超越，但在某些时序数据和流式处理场景中仍有应用。","RNN"
"什么是序列到序列 (Seq2Seq) 模型？","Seq2Seq模型是一种基于RNN（或Transformer）的架构，由一个编码器（Encoder）和一个解码器（Decoder）组成。编码器负责将输入序列压缩成一个固定长度的上下文向量（context vector），解码器则根据这个上下文向量生成输出序列。它非常适合处理输入和输出序列长度不等的任务，如机器翻译。","RNN"