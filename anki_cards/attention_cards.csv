"Front","Back"
"什么是注意力机制 (Attention Mechanism)？","注意力机制是深度学习中的一项革命性技术，其核心思想是让模型在处理信息时能够""关注""最相关的部分，动态地为输入的不同部分分配不同的权重。"
"注意力机制的生动类比是什么？","就像在嘈杂的派对上，你的大脑会自动""聚焦""在朋友的声音上，过滤掉背景噪音。注意力机制就像一个智能的""聚光灯""，在处理信息时动态地照亮最重要的部分。"
"注意力机制中的Q, K, V分别代表什么？","Q (Query)代表""我想要关注什么""，是当前处理的信息向量。K (Key)代表""我能提供什么""，用于匹配查询。V (Value)代表""我的实际内容""，是真正被提取的信息向量。"
"注意力机制的工作流程是怎样的？","1. 生成Q, K, V向量。 2. 计算Query和Key的相似度得到注意力分数。 3. 对分数应用Softmax函数得到注意力权重。 4. 用注意力权重对Value进行加权求和得到最终输出。"
"什么是自注意力 (Self-Attention)？","序列中的每个元素都与序列中所有元素（包括自己）进行交互，捕捉序列内部的依赖关系。Query、Key和Value都来自同一序列。"
"什么是交叉注意力 (Cross-Attention)？","Query来自一个序列，而Key和Value来自另一个序列。它用于建立两个不同序列之间的关系，常用于机器翻译和图像描述生成等任务。"
"什么是多头注意力 (Multi-Head Attention)？","并行运行多组独立的注意力机制（称为""头""），每个头学习输入信息的不同表示子空间，然后将所有头的结果拼接起来。这使得模型能从多个角度理解输入。"
"什么是掩码注意力 (Masked Attention)？","在计算注意力时屏蔽（mask）掉某些位置，以防止信息泄露。在自回归生成任务（如语言模型）中至关重要，确保模型在生成当前词时只能看到之前的词。"
"注意力机制相比于RNN有哪些优势？","1. **并行计算**：不像RNN需要顺序处理，注意力机制可以并行处理整个序列。 2. **长距离依赖**：可以直接建立任意两个位置间的联系，路径长度为O(1)，有效解决了RNN的长期依赖问题。"
"标准注意力机制的主要挑战是什么？","它的计算复杂度和内存消耗都是O(n²)，其中n是序列长度。这使得它在处理非常长的序列时开销巨大。"
"为了解决标准注意力的计算复杂度问题，有哪些优化变体？","Sparse Attention, Linformer, Performer, FlashAttention, Longformer, Sliding Window Attention等，它们通过稀疏化、低秩近似、核方法或优化内存访问等方式将复杂度降低到O(n√n)或O(n)。"
"注意力机制在哪些领域有广泛应用？","大语言模型 (LLM)、机器翻译、计算机视觉 (Vision Transformer)、推荐系统、语音技术、多模态AI等。"
